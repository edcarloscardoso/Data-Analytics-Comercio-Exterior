# ğŸ“¦ Data Analytics ComÃ©rcio Exterior

## ğŸ¯ Objetivo

Este projeto tem como foco o tratamento e organizaÃ§Ã£o de dados de exportaÃ§Ã£o do Brasil, utilizando a **Arquitetura de MedalhÃ£o** (Medallion Architecture) para estruturar os dados em camadas: **raw**, **landing**, **silver** e **gold**. A proposta Ã© garantir integridade, qualidade e disponibilidade dos dados para anÃ¡lises futuras.

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

O ambiente foi montado utilizando **WSL2 (Ubuntu)** e **VS Code**, com o seguinte setup:

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/WSL2
code .
pip install requests pandas
```

---

## ğŸ“… Fonte de Dados

Os dados utilizados neste projeto foram obtidos da **base pÃºblica do MinistÃ©rio da IndÃºstria, ComÃ©rcio Exterior e ServiÃ§os (MDIC)**.

### Bases Utilizadas:
- **NCM** (Nomenclatura Comum do Mercosul): 2023 e 2024
- **SH4** (Sistema Harmonizado): 2023 e 2024
- **URF**, **PaÃ­s**, **Unidade**, **Via de Transporte**, **NCM Unidade**

Essas bases contÃªm informaÃ§Ãµes detalhadas sobre o comÃ©rcio exterior brasileiro, como categorias de mercadorias, unidades de medida, e destinos.

https://www.gov.br/mdic/pt-br/assuntos/comercio-exterior/estatisticas/base-de-dados-bruta

---

## ğŸ§ª Processamento e TransformaÃ§Ãµes

O processo de transformaÃ§Ã£o dos dados seguiu as seguintes etapas, aplicando funÃ§Ãµes importantes do `pandas`:

### ğŸ”¹ Leitura dos Arquivos CSV
- UtilizaÃ§Ã£o de `pandas.read_csv()`
- ConfiguraÃ§Ãµes:
  - `encoding='latin1'` para caracteres especiais
  - `sep=';'` para arquivos do MDIC

### ğŸ”¹ PadronizaÃ§Ã£o dos Dados
- ConversÃ£o de colunas-chave para `string`:
```python
df["cd_pais"] = df["cd_pais"].astype(str)
df["cd_urf"] = df["cd_urf"].astype(str)
df["cd_ncm"] = df["cd_ncm"].astype(str)
```
- RenomeaÃ§Ã£o e limpeza de colunas:
```python
df.columns = df.columns.str.lower().str.strip().str.replace(" ", "_")
df.rename(columns={"NO_PAIS": "cd_pais", "NO_NCM_POR": "cd_ncm"}, inplace=True)
```

### ğŸ”¹ SubstituiÃ§Ã£o de CÃ³digos por DescriÃ§Ãµes (MERGE)
- Para tornar os dados mais legÃ­veis, foram feitas junÃ§Ãµes usando `merge()`:
```python
df_merge = df_export.merge(df_paises, left_on='cd_pais', right_on='CO_PAIS', how='left')
```
- Essa tÃ©cnica foi aplicada tambÃ©m para unir informaÃ§Ãµes de URF, unidade de medida, via de transporte e NCM.

### ğŸ”¹ ExtraÃ§Ã£o e SeparaÃ§Ã£o de InformaÃ§Ãµes
- A coluna `NO_URF`, por exemplo, foi tratada para separar tipo e nome com `str.extract()` e `str.replace()`:
```python
df['urf_codigo'] = df['NO_URF'].str.extract(r'^(\d+)\s*-\s*')
df['urf_info'] = df['NO_URF'].str.replace(r'^\d+\s*-\s*', '', regex=True)
```

### ğŸ”¹ PadronizaÃ§Ã£o de Valores Textuais
- Foi criada uma funÃ§Ã£o customizada para padronizar textos:
```python
def padronizar_colunas(df, colunas):
    df[colunas] = df[colunas].apply(lambda x: x.str.strip().str.lower())
    return df
```

### ğŸ”¹ RemoÃ§Ã£o de Colunas e Limpeza
```python
df.drop(columns=['coluna_irrelevante'], inplace=True)
df.dropna(inplace=True)
df.drop_duplicates(inplace=True)
```

---

## ğŸ§± Arquitetura de MedalhÃ£o

### ğŸŸ¤ RAW
- Download automÃ¡tico dos arquivos com `requests` e salvamento inicial em `raw_data`.

### ğŸŸ¡ LANDING
- PadronizaÃ§Ã£o bÃ¡sica: renomeaÃ§Ã£o de colunas, tipos e criaÃ§Ã£o da coluna `data_exportacao` a partir de ano e mÃªs.

### âšª SILVER
- RealizaÃ§Ã£o de merges com tabelas auxiliares para substituir cÃ³digos por nomes legÃ­veis.
- PadronizaÃ§Ã£o geral das colunas e eliminaÃ§Ã£o de colunas desnecessÃ¡rias.

### ğŸŸ¢ GOLD
- ConsolidaÃ§Ã£o dos dados: JunÃ§Ã£o dos dados de exportaÃ§Ã£o e importaÃ§Ã£o de diferentes anos e fontes, usando pd.concat(), resultando em tabelas mestre consolidadas para cada fluxo.
- Enriquecimento e AgregaÃ§Ã£o: AplicaÃ§Ã£o de transformaÃ§Ãµes finais e agregaÃ§Ãµes para gerar conjuntos de dados prontos para anÃ¡lise e visualizaÃ§Ã£o. Isso inclui:
- Agrupamento por entidades chave: AgregaÃ§Ãµes por paÃ­s, NCM, via de transporte, etc.
- CÃ¡lculo de mÃ©tricas: Soma de valores FOB, peso lÃ­quido, e outras medidas relevantes.
- CriaÃ§Ã£o de indicadores: CÃ¡lculo de mÃ©dias percentuais (ex: frete/seguro sobre valor FOB).

GeraÃ§Ã£o de Insights Chave:

ExportaÃ§Ã£o:
- Top Produtos por Valor (FOB): IdentificaÃ§Ã£o dos produtos (NCM) que geraram maior receita em dÃ³lares FOB.
- EvoluÃ§Ã£o Mensal das ExportaÃ§Ãµes: AnÃ¡lise do volume (kg) e valor (USD) total das exportaÃ§Ãµes ao longo do tempo.
- Modal de Transporte: DistribuiÃ§Ã£o e importÃ¢ncia das diferentes vias de transporte utilizadas.
- ExportaÃ§Ãµes por Localidade: Soma do valor e peso das exportaÃ§Ãµes originadas de diferentes municÃ­pios e estados.
- Volume Exportado por NCM: Total de peso (kg) exportado para cada categoria de produto.

ImportaÃ§Ã£o:
- ImportaÃ§Ã£o por Estado e Ano: AnÃ¡lise do valor total importado por unidade federativa ao longo dos anos.
- Top Produtos Importados por Valor (FOB): IdentificaÃ§Ã£o dos produtos (NCM) com maior valor de importaÃ§Ã£o.
- Percentual de Frete e Seguro: CÃ¡lculo da mÃ©dia percentual dos custos de frete e seguro em relaÃ§Ã£o ao valor FOB da importaÃ§Ã£o.
- Principais PaÃ­ses Fornecedores: IdentificaÃ§Ã£o dos paÃ­ses com maior valor total de importaÃ§Ã£o.
- EvoluÃ§Ã£o Mensal das ImportaÃ§Ãµes: AnÃ¡lise do volume e valor total das importaÃ§Ãµes ao longo do tempo.
- Os dados nesta camada estÃ£o limpos, estruturados e agregados, prontos para consumo por ferramentas de BI, dashboards e anÃ¡lises mais aprofundadas.

---

## ğŸ—‚ï¸ Estrutura de Pastas

```
Data-Analytics-Comercio-Exterior/
â”œâ”€â”€ datalake/
â”‚   â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ raw_data/
â”‚   â””â”€â”€ silver/
â”œâ”€â”€ workflow/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ process/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ exportacao/
â”‚       â”‚   â””â”€â”€ importacao/
â”‚       â”œâ”€â”€ gold/
â”‚       â””â”€â”€ silver/
â”‚           â”œâ”€â”€ exportacao_silver/
â”‚           â”œâ”€â”€ importacao_silver/
â”‚           â””â”€â”€ transformacao_data_gold/
â”œâ”€â”€ ingestao.md
â””â”€â”€ README.md
```

---

## ğŸ’¡ AutomatizaÃ§Ã£o de carregagamento de arquivos
- Fiz um script que automatiza o processo de carregamento de arquivos .csv de uma pasta local para um banco de dados PostgresSQL.


### ğŸ”¹ Listagem e Leitura dos Arquivos CSV

A automaÃ§Ã£o inicia com a varredura da pasta `datalake/gold`, onde ficam armazenados os arquivos `.csv` prontos para carga. Utilizamos a biblioteca `os` para identificar os arquivos e garantir que apenas arquivos vÃ¡lidos fossem processados.

A leitura dos dados Ã© feita com a biblioteca `pandas`, com foco inicial em uma amostra parcial de cada arquivo (cerca de 100 linhas), apenas para detectar a estrutura das colunas. Isso permitiu criar a tabela no banco de dados antes de inserir os dados por completo. Para garantir a correta interpretaÃ§Ã£o de caracteres especiais, foi utilizado o encoding `latin1`.

---

### ğŸ”¹ CriaÃ§Ã£o das Tabelas no Banco de Dados

Antes da carga completa, Ã© necessÃ¡rio garantir que o banco esteja preparado para receber os dados. A biblioteca `SQLAlchemy` foi empregada para criar as tabelas no PostgreSQL de forma dinÃ¢mica, baseando-se no cabeÃ§alho de cada CSV.

As colunas foram padronizadas como texto (`string`) para evitar conflitos de tipo e permitir maior flexibilidade nos dados. O nome de cada tabela Ã© gerado automaticamente com base no nome do arquivo correspondente. Se a tabela jÃ¡ existir, ela Ã© substituÃ­da â€” uma abordagem simples e eficaz para manter os dados atualizados.

---

### ğŸ”¹ Carga Otimizada dos Dados com COPY

Para a etapa de inserÃ§Ã£o dos dados, foi utilizada a biblioteca `psycopg2`, que permite acesso direto ao banco PostgreSQL via Python. O destaque aqui Ã© o uso do comando nativo `COPY`, uma das formas mais rÃ¡pidas e eficientes de carregar grandes volumes de dados em uma tabela.

Essa abordagem foi escolhida por sua performance superior em comparaÃ§Ã£o a inserÃ§Ãµes tradicionais linha a linha. O comando `COPY` lÃª diretamente o conteÃºdo do arquivo `.csv` e insere no banco, respeitando o cabeÃ§alho e a estrutura da tabela previamente criada.

---

### ğŸ”¹ Controle de TransaÃ§Ãµes e Tratamento de Erros

Cada carga de arquivo Ã© envolvida em uma transaÃ§Ã£o. Caso ocorra qualquer erro durante o processo, a operaÃ§Ã£o Ã© revertida automaticamente com `rollback`, mantendo a integridade do banco de dados. 

Com isso, mesmo que um arquivo apresente problemas, os demais continuam sendo processados normalmente â€” uma estratÃ©gia importante para ambientes de dados em produÃ§Ã£o.

---

### ğŸ”¹ Encerramento e Feedback

ApÃ³s o processamento de todos os arquivos, a conexÃ£o com o banco de dados Ã© encerrada corretamente. O script fornece mensagens no terminal indicando o andamento e a conclusÃ£o de cada etapa, facilitando o acompanhamento da carga.

---

Este processo garante uma automaÃ§Ã£o robusta, segura e escalÃ¡vel para inserÃ§Ã£o de dados no PostgreSQL, aproveitando o melhor de ferramentas como `pandas`, `SQLAlchemy` e `psycopg2`.

Com esse processo foi possÃ­vel conectar o banco de dado com a ferramenta de Business Intelligence (BI) open-source para exploraÃ§Ã£o, visualizaÃ§Ã£o e compartilhamento dos dados.

---

## ğŸ’¡ Melhorias Futuras
- ImplementaÃ§Ã£o de dashboards interativos
- ConexÃ£o com ferramentas de BI (Power BI e Metabase)

---

## ğŸ“Œ Status Atual
- âœ… Raw, Landing, Silver, Gold concluÃ­dos e automatiÃ§Ã£o com banco de dados.

---

Sinta-se Ã  vontade para explorar, usar ou contribuir com o projeto!
